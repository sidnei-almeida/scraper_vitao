import requests
from bs4 import BeautifulSoup
import json
import time
import os
from urllib.parse import urljoin

class VitaoUrlCollector:
    def __init__(self):
        self.base_url = "https://www.fatsecret.com.br"
        self.search_url = "https://www.fatsecret.com.br/calorias-nutri%C3%A7%C3%A3o/search?q=Vitao"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.output_file = "dados/vitao_urls.json"
        self.collected_urls = []
        
    def get_page_content(self, url):
        """Faz a requisi√ß√£o HTTP e retorna o conte√∫do da p√°gina"""
        try:
            response = requests.get(url, headers=self.headers)
            response.raise_for_status()
            return response.text
        except requests.RequestException as e:
            print(f"Erro ao acessar {url}: {e}")
            return None
    
    def extract_urls_from_page(self, soup):
        """Extrai URLs dos produtos da p√°gina atual"""
        urls = []
        
        # Busca por links com classe 'prominent' que s√£o os produtos
        product_links = soup.find_all('a', class_='prominent')
        
        for link in product_links:
            href = link.get('href')
            if href and '/calorias-nutri%C3%A7%C3%A3o/vitao/' in href:
                # Constr√≥i a URL completa
                full_url = urljoin(self.base_url, href)
                urls.append(full_url)
                print(f"  ‚úì Encontrada: {link.get_text(strip=True)}")
        
        return urls
    
    def check_no_results(self, soup):
        """Verifica se a p√°gina n√£o tem resultados"""
        no_results = soup.find('div', class_='searchNoResult')
        return no_results is not None
    
    def collect_all_urls(self):
        """Coleta todas as URLs dos produtos da Vitao"""
        print("üîç Iniciando coleta de URLs dos produtos da Vitao...")
        
        page = 0
        total_urls = 0
        
        while True:
            # Constr√≥i a URL da p√°gina
            if page == 0:
                page_url = self.search_url
            else:
                page_url = f"{self.search_url}&pg={page}"
            
            print(f"\nüìÑ Processando p√°gina {page + 1}...")
            print(f"URL: {page_url}")
            
            # Obt√©m o conte√∫do da p√°gina
            content = self.get_page_content(page_url)
            if not content:
                print("‚ùå Erro ao acessar a p√°gina")
                break
            
            # Parse do HTML
            soup = BeautifulSoup(content, 'html.parser')
            
            # Verifica se n√£o h√° resultados
            if self.check_no_results(soup):
                print("üèÅ Nenhum resultado encontrado. Coleta finalizada.")
                break
            
            # Extrai URLs da p√°gina atual
            page_urls = self.extract_urls_from_page(soup)
            
            if not page_urls:
                print("‚ö†Ô∏è  Nenhuma URL encontrada nesta p√°gina")
                break
            
            # Adiciona URLs √† lista
            self.collected_urls.extend(page_urls)
            total_urls += len(page_urls)
            
            print(f"‚úÖ {len(page_urls)} URLs coletadas da p√°gina {page + 1}")
            print(f"üìä Total acumulado: {total_urls} URLs")
            
            # Pausa entre requisi√ß√µes
            time.sleep(2)
            
            # Avan√ßa para a pr√≥xima p√°gina
            page += 1
        
        print(f"\nüéâ Coleta finalizada! Total de {len(self.collected_urls)} URLs coletadas.")
        return self.collected_urls
    
    def save_urls_to_json(self):
        """Salva as URLs coletadas em um arquivo JSON"""
        # Cria o diret√≥rio se n√£o existir
        os.makedirs(os.path.dirname(self.output_file), exist_ok=True)
        
        # Remove URLs duplicadas
        unique_urls = list(set(self.collected_urls))
        
        # Salva no arquivo JSON
        with open(self.output_file, 'w', encoding='utf-8') as jsonfile:
            json.dump(unique_urls, jsonfile, indent=2, ensure_ascii=False)
        
        print(f"üíæ URLs salvas em: {self.output_file}")
        print(f"üìù {len(unique_urls)} URLs √∫nicas salvas")
        
        return unique_urls
    
    def run(self):
        """Executa o processo completo de coleta"""
        # Coleta todas as URLs
        self.collect_all_urls()
        
        if self.collected_urls:
            # Salva no arquivo JSON
            unique_urls = self.save_urls_to_json()
            
            # Mostra algumas URLs como exemplo
            print("\nüìã Exemplos de URLs coletadas:")
            for i, url in enumerate(unique_urls[:5], 1):
                print(f"  {i}. {url}")
            
            if len(unique_urls) > 5:
                print(f"  ... e mais {len(unique_urls) - 5} URLs")
        else:
            print("‚ùå Nenhuma URL foi coletada")

def main():
    """Fun√ß√£o principal"""
    collector = VitaoUrlCollector()
    collector.run()

if __name__ == "__main__":
    main() 